{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySpart Practice",
      "provenance": [],
      "authorship_tag": "ABX9TyNEpPcw1197rBxDP1ByIo91",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jesudhas/Data-science/blob/master/PySpart_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuBMK0bAaYJG"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-C8OYVxajlh"
      },
      "source": [
        "!wget -q http://apache.osuosl.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8FRi_ndamEp"
      },
      "source": [
        "!tar xf spark-3.0.1-bin-hadoop3.2.tgz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9StwfKNUau-E"
      },
      "source": [
        "!pip install -q findspark"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8JW1qp8azP9"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop3.2\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVttwn7na4gd",
        "outputId": "8a0eb9e5-04ea-4d32-c77a-0a1e3f1b3103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# Test the spark \n",
        "df = spark.createDataFrame([{\"hello\": \"world\"} for x in range(1000)])\n",
        "\n",
        "df.show(10, False)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/session.py:381: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
            "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "|hello|\n",
            "+-----+\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RqgTwMka8r9",
        "outputId": "6671fe35-aa57-4a3a-d75a-e4e99b0fe092",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "\n",
        "sentenceData = spark.createDataFrame([\n",
        "    (0.0, \"Hi I heard about Spark\"),\n",
        "    (0.0, \"I wish Java could use case classes\"),\n",
        "    (1.0, \"Logistic regression models are neat\")\n",
        "], [\"label\", \"sentence\"])\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "wordsData = tokenizer.transform(sentenceData)\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
        "featurizedData = hashingTF.transform(wordsData)\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
        "\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "idfModel = idf.fit(featurizedData)\n",
        "rescaledData = idfModel.transform(featurizedData)\n",
        "\n",
        "rescaledData.select(\"label\", \"features\").show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(20,[6,8,13,16],[...|\n",
            "|  0.0|(20,[0,2,7,13,15,...|\n",
            "|  1.0|(20,[3,4,6,11,19]...|\n",
            "+-----+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WL8lfVksdHWN",
        "outputId": "28e5753b-fbbd-4bc1-d16d-b30f8bd77400",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "from pyspark.ml.feature import MinMaxScaler\n",
        "from pyspark.ml.linalg import Vectors\n",
        "# Create some dummy feature data\n",
        "features_df = spark.createDataFrame([\n",
        "    (1, Vectors.dense([10.0,10000.0,1.0]),),\n",
        "    (2, Vectors.dense([20.0,30000.0,2.0]),),\n",
        "    (3, Vectors.dense([30.0,40000.0,3.0]),),\n",
        "    \n",
        "],[\"id\", \"features\"] )\n",
        "features_df.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------------------+\n",
            "| id|          features|\n",
            "+---+------------------+\n",
            "|  1|[10.0,10000.0,1.0]|\n",
            "|  2|[20.0,30000.0,2.0]|\n",
            "|  3|[30.0,40000.0,3.0]|\n",
            "+---+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwwo1IhQdppO",
        "outputId": "adba7240-3546-42f9-95e2-02fd280e9d43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "features_scaler = MinMaxScaler(inputCol = \"features\", outputCol = \"sfeatures\")\n",
        "smodel = features_scaler.fit(features_df)\n",
        "sfeatures_df = smodel.transform(features_df)\n",
        "sfeatures_df.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------------------+--------------------+\n",
            "| id|          features|           sfeatures|\n",
            "+---+------------------+--------------------+\n",
            "|  1|[10.0,10000.0,1.0]|           (3,[],[])|\n",
            "|  2|[20.0,30000.0,2.0]|[0.5,0.6666666666...|\n",
            "|  3|[30.0,40000.0,3.0]|       [1.0,1.0,1.0]|\n",
            "+---+------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vazruOIgjPa"
      },
      "source": [
        "Standardize Numeric Data\n",
        "StandardScaler is another well-known class written with machine learning libraries. It normalizes the data between -1 and 1 and converts the data into bell-shaped data. You can demean the data and scale to some variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdsnNTKOem0W"
      },
      "source": [
        "from pyspark.ml.feature import  StandardScaler\n",
        "from pyspark.ml.linalg import Vectors"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAxYBUf9gDPN"
      },
      "source": [
        "# Create the dummy data\n",
        "features_df = spark.createDataFrame([\n",
        "    (1, Vectors.dense([10.0,10000.0,1.0]),),\n",
        "    (2, Vectors.dense([20.0,30000.0,2.0]),),\n",
        "    (3, Vectors.dense([30.0,40000.0,3.0]),),\n",
        "    \n",
        "],[\"id\", \"features\"] )"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AZQs1xXgH09",
        "outputId": "dba1f0a8-0fd3-4909-b8b5-7e4d3d8a0dcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Apply the StandardScaler model\n",
        "features_stand_scaler = StandardScaler(inputCol = \"features\", outputCol = \"sfeatures\", withStd=True, withMean=True)\n",
        "stmodel = features_stand_scaler.fit(features_df)\n",
        "stand_sfeatures_df = stmodel.transform(features_df)\n",
        "stand_sfeatures_df.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------------------+--------------------+\n",
            "| id|          features|           sfeatures|\n",
            "+---+------------------+--------------------+\n",
            "|  1|[10.0,10000.0,1.0]|[-1.0,-1.09108945...|\n",
            "|  2|[20.0,30000.0,2.0]|[0.0,0.2182178902...|\n",
            "|  3|[30.0,40000.0,3.0]|[1.0,0.8728715609...|\n",
            "+---+------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJZujG4NgnEC"
      },
      "source": [
        "Bucketize Numeric Data\n",
        "The real data sets come with various ranges and sometimes it is advisable to transform the data into well-defined buckets before plugging into machine learning algorithms.\n",
        "Bucketizer class is handy to transform the data into various buckets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nobjRi4gSfu",
        "outputId": "dbc2586d-fb6e-46e6-f34a-1cf43a763774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from pyspark.ml.feature import  Bucketizer\n",
        "from pyspark.ml.linalg import Vectors\n",
        "# Define the splits for buckets\n",
        "splits = [-float(\"inf\"), -10, 0.0, 10, float(\"inf\")]\n",
        "b_data = [(-800.0,), (-10.5,), (-1.7,), (0.0,), (8.2,), (90.1,)]\n",
        "b_df = spark.createDataFrame(b_data, [\"features\"])\n",
        "b_df.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+\n",
            "|features|\n",
            "+--------+\n",
            "|  -800.0|\n",
            "|   -10.5|\n",
            "|    -1.7|\n",
            "|     0.0|\n",
            "|     8.2|\n",
            "|    90.1|\n",
            "+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PydxwQu_kViE",
        "outputId": "c6490434-5d4f-43b7-b6f5-ce6df67929e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Transforming data into buckets\n",
        "bucketizer = Bucketizer(splits=splits, inputCol= \"features\", outputCol=\"bfeatures\")\n",
        "bucketed_df = bucketizer.transform(b_df)\n",
        "bucketed_df.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+---------+\n",
            "|features|bfeatures|\n",
            "+--------+---------+\n",
            "|  -800.0|      0.0|\n",
            "|   -10.5|      0.0|\n",
            "|    -1.7|      1.0|\n",
            "|     0.0|      2.0|\n",
            "|     8.2|      2.0|\n",
            "|    90.1|      3.0|\n",
            "+--------+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apnu5kOAg6l8",
        "outputId": "e85fd542-4c3b-4a41-d636-79f6a0cde451",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "# Input data: Each row is a bag of words with a ID.\n",
        "df = spark.createDataFrame([\n",
        "    (0, \"a b c\".split(\" \")),\n",
        "    (1, \"a b b c a\".split(\" \"))\n",
        "], [\"id\", \"words\"])\n",
        "\n",
        "# fit a CountVectorizerModel from the corpus.\n",
        "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n",
        "\n",
        "model = cv.fit(df)\n",
        "\n",
        "result = model.transform(df)\n",
        "result.show(truncate=False)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---------------+-------------------------+\n",
            "|id |words          |features                 |\n",
            "+---+---------------+-------------------------+\n",
            "|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
            "|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
            "+---+---------------+-------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc6FuuEkj-8N",
        "outputId": "cb766e5d-0c0d-4a5c-b7cd-b2a5b7dbfc27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "from pyspark.ml.feature import  Tokenizer\n",
        "sentences_df = spark.createDataFrame([\n",
        "    (1, \"This is an introduction to sparkMlib\"),\n",
        "    (2, \"Mlib incluse libraries for classfication and regression\"),\n",
        "    (3, \"It also incluses support for data piple lines\"),\n",
        "    (4, \"It is good for health, food is important\"),\n",
        "    \n",
        "], [\"id\", \"sentences\"])\n",
        "sentences_df.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------------------+\n",
            "| id|           sentences|\n",
            "+---+--------------------+\n",
            "|  1|This is an introd...|\n",
            "|  2|Mlib incluse libr...|\n",
            "|  3|It also incluses ...|\n",
            "|  4|It is good for he...|\n",
            "+---+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-jCEvMQl1K2"
      },
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpkk3ufGmHHd"
      },
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans, BisectingKMeans\n",
        "import glob\n",
        "# Downloading the clustering dataset\n",
        "!wget -q 'https://raw.githubusercontent.com/amjadraza/blogs-data/master/spark_ml/clustering_dataset.csv'"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJfeRnKLm22N"
      },
      "source": [
        "# Read the data.\n",
        "clustering_file_name ='clustering_dataset.csv'\n",
        "import pandas as pd\n",
        "# df = pd.read_csv(clustering_file_name)\n",
        "cluster_df = spark.read.csv(clustering_file_name, header=True,inferSchema=True)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swy5QWOJm9il",
        "outputId": "18bafd60-f806-4451-a3d1-0de6079584c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# Coverting the input data into features column\n",
        "vectorAssembler = VectorAssembler(inputCols = ['col1', 'col2', 'col3'], outputCol = \"features\")\n",
        "vcluster_df = vectorAssembler.transform(cluster_df)\n",
        "vcluster_df.show(10)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+----+----+--------------+\n",
            "|col1|col2|col3|      features|\n",
            "+----+----+----+--------------+\n",
            "|   7|   4|   1| [7.0,4.0,1.0]|\n",
            "|   7|   7|   9| [7.0,7.0,9.0]|\n",
            "|   7|   9|   6| [7.0,9.0,6.0]|\n",
            "|   1|   6|   5| [1.0,6.0,5.0]|\n",
            "|   6|   7|   7| [6.0,7.0,7.0]|\n",
            "|   7|   9|   4| [7.0,9.0,4.0]|\n",
            "|   7|  10|   6|[7.0,10.0,6.0]|\n",
            "|   7|   8|   2| [7.0,8.0,2.0]|\n",
            "|   8|   3|   8| [8.0,3.0,8.0]|\n",
            "|   4|  10|   5|[4.0,10.0,5.0]|\n",
            "+----+----+----+--------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA3Y03ePnLLW"
      },
      "source": [
        "# Applying the k-means algorithm\n",
        "kmeans = KMeans().setK(3)\n",
        "kmeans = kmeans.setSeed(1)\n",
        "kmodel = kmeans.fit(vcluster_df)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dmg5A8QMnZu2",
        "outputId": "dcfd0a28-7175-4c75-efbc-9d5747839e83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "centers = kmodel.clusterCenters()\n",
        "print(\"The location of centers: {}\".format(centers))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The location of centers: [array([35.88461538, 31.46153846, 34.42307692]), array([80.        , 79.20833333, 78.29166667]), array([5.12, 5.84, 4.84])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGvNPgRRni7U",
        "outputId": "4240ca2a-f1a1-4e21-f409-71c68c5b40da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Applying Hierarchical Clustering\n",
        "bkmeans = BisectingKMeans().setK(3)\n",
        "bkmeans = bkmeans.setSeed(1)\n",
        "bkmodel = bkmeans.fit(vcluster_df)\n",
        "bkcneters = bkmodel.clusterCenters()\n",
        "bkcneters"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([5.12, 5.84, 4.84]),\n",
              " array([35.88461538, 31.46153846, 34.42307692]),\n",
              " array([80.        , 79.20833333, 78.29166667])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkEMJ2QZnqLV"
      },
      "source": [
        "# Downloading the clustering data\n",
        "!wget -q \"https://raw.githubusercontent.com/amjadraza/blogs-data/master/spark_ml/iris.csv\""
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocTwMzgfoKEw"
      },
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/amjadraza/blogs-data/master/spark_ml/iris.csv\", header=None)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcG_djbsoOCN",
        "outputId": "d1ed8552-e6ae-407b-cf4b-f6d59b7be964",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     0    1    2    3            4\n",
              "0  5.1  3.5  1.4  0.2  Iris-setosa\n",
              "1  4.9  3.0  1.4  0.2  Iris-setosa\n",
              "2  4.7  3.2  1.3  0.2  Iris-setosa\n",
              "3  4.6  3.1  1.5  0.2  Iris-setosa\n",
              "4  5.0  3.6  1.4  0.2  Iris-setosa"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPpbe_bDoQJ9",
        "outputId": "3642bd31-327a-4538-e79e-08283eaa8475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "spark.createDataFrame(df)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[0: double, 1: double, 2: double, 3: double, 4: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft5Dei_tosCe"
      },
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import  StringIndexer"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsxi1u6Ypepd",
        "outputId": "b5f2c06f-4a40-4b3e-ce36-a3b3b9cb214b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Read the iris data\n",
        "df_iris = pd.read_csv(\"https://raw.githubusercontent.com/amjadraza/blogs-data/master/spark_ml/iris.csv\", header=None)\n",
        "iris_df = spark.createDataFrame(df_iris)\n",
        "iris_df.show(5, False)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+---+---+-----------+\n",
            "|0  |1  |2  |3  |4          |\n",
            "+---+---+---+---+-----------+\n",
            "|5.1|3.5|1.4|0.2|Iris-setosa|\n",
            "|4.9|3.0|1.4|0.2|Iris-setosa|\n",
            "|4.7|3.2|1.3|0.2|Iris-setosa|\n",
            "|4.6|3.1|1.5|0.2|Iris-setosa|\n",
            "|5.0|3.6|1.4|0.2|Iris-setosa|\n",
            "+---+---+---+---+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8iusgC8pg4N"
      },
      "source": [
        " #Rename the columns\n",
        "iris_df = iris_df.select(col(\"0\").alias(\"sepal_length\"),\n",
        "                         col(\"1\").alias(\"sepal_width\"),\n",
        "                         col(\"2\").alias(\"petal_length\"),\n",
        "                         col(\"3\").alias(\"petal_width\"),\n",
        "                         col(\"4\").alias(\"species\"),\n",
        "                        )"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj0Jfb_opp1N",
        "outputId": "7c502057-7f8a-4c93-8d92-51c4b1f022cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Converting the columns into features\n",
        "vectorAssembler = VectorAssembler(inputCols = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"],\n",
        "                                  outputCol = \"features\")\n",
        "viris_df = vectorAssembler.transform(iris_df)\n",
        "viris_df.show(5, False)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+-----------+------------+-----------+-----------+-----------------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species    |features         |\n",
            "+------------+-----------+------------+-----------+-----------+-----------------+\n",
            "|5.1         |3.5        |1.4         |0.2        |Iris-setosa|[5.1,3.5,1.4,0.2]|\n",
            "|4.9         |3.0        |1.4         |0.2        |Iris-setosa|[4.9,3.0,1.4,0.2]|\n",
            "|4.7         |3.2        |1.3         |0.2        |Iris-setosa|[4.7,3.2,1.3,0.2]|\n",
            "|4.6         |3.1        |1.5         |0.2        |Iris-setosa|[4.6,3.1,1.5,0.2]|\n",
            "|5.0         |3.6        |1.4         |0.2        |Iris-setosa|[5.0,3.6,1.4,0.2]|\n",
            "+------------+-----------+------------+-----------+-----------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0Ooc5g4pvue",
        "outputId": "73ca440f-37b4-485f-b98f-8eebde47ec16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "indexer = StringIndexer(inputCol=\"species\", outputCol = \"label\")\n",
        "iviris_df = indexer.fit(viris_df).transform(viris_df)\n",
        "iviris_df.show(2, False)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+-----------+------------+-----------+-----------+-----------------+-----+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species    |features         |label|\n",
            "+------------+-----------+------------+-----------+-----------+-----------------+-----+\n",
            "|5.1         |3.5        |1.4         |0.2        |Iris-setosa|[5.1,3.5,1.4,0.2]|0.0  |\n",
            "|4.9         |3.0        |1.4         |0.2        |Iris-setosa|[4.9,3.0,1.4,0.2]|0.0  |\n",
            "+------------+-----------+------------+-----------+-----------+-----------------+-----+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMtAr8GLqURR"
      },
      "source": [
        "Naive Bayes Classification\n",
        "Once the data is prepared, we are ready to apply the first classification algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKpQxSt8qLWt"
      },
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "# Create the traing and test splits\n",
        "splits = iviris_df.randomSplit([0.6,0.4], 1)\n",
        "train_df = splits[0]\n",
        "test_df = splits[1]"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSRUU95xqXJc",
        "outputId": "6ee03854-a2f4-4b10-bbd9-99354593817c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Apply the Naive bayes classifier\n",
        "nb = NaiveBayes(modelType=\"multinomial\")\n",
        "nbmodel = nb.fit(train_df)\n",
        "predictions_df = nbmodel.transform(test_df)\n",
        "predictions_df.show(1, False)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+-----------+------------+-----------+-----------+-----------------+-----+------------------------------------------------------------+------------------------------------------------------------+----------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species    |features         |label|rawPrediction                                               |probability                                                 |prediction|\n",
            "+------------+-----------+------------+-----------+-----------+-----------------+-----+------------------------------------------------------------+------------------------------------------------------------+----------+\n",
            "|4.3         |3.0        |1.1         |0.1        |Iris-setosa|[4.3,3.0,1.1,0.1]|0.0  |[-9.966434726497221,-11.294595492758821,-11.956012812323921]|[0.7134106367667451,0.18902823898426235,0.09756112424899269]|0.0       |\n",
            "+------------+-----------+------------+-----------+-----------+-----------------+-----+------------------------------------------------------------+------------------------------------------------------------+----------+\n",
            "only showing top 1 row\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sBjOuKQq-Px"
      },
      "source": [
        "Multilayer Perceptron Classification\n",
        "The second classifier we will be investigating is a Multi-layer perceptron. In this tutorial, I am not going into details of the optimal MLP network for this problem however in practice, you research the optimal network suitable to the problem in hand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viYPFZvQqqSN",
        "outputId": "af3ba373-0fa9-406b-81d1-c8b837ec9fbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "# Define the MLP Classifier\n",
        "layers = [4,5,5,3]\n",
        "mlp = MultilayerPerceptronClassifier(layers = layers, seed=1)\n",
        "mlp_model = mlp.fit(train_df)\n",
        "mlp_predictions = mlp_model.transform(test_df)\n",
        "# Evaluate the MLP classifier\n",
        "mlp_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "mlp_accuracy = mlp_evaluator.evaluate(mlp_predictions)\n",
        "mlp_accuracy"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9827586206896551"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3lL9rfHrAJD"
      },
      "source": [
        "Decision Trees Classification\n",
        "Another common classifier in the ML family is the Decision Tree Classifier, in this section, we explore this classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwjqFtyZq4rl",
        "outputId": "facfd483-252c-42da-e5e6-a60d0f08e185",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "# Define the DT Classifier \n",
        "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
        "dt_model = dt.fit(train_df)\n",
        "dt_predictions = dt_model.transform(test_df)\n",
        "# Evaluate the DT Classifier\n",
        "dt_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "dt_accuracy = dt_evaluator.evaluate(dt_predictions)\n",
        "dt_accuracy"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9827586206896551"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZywN19VMrOko"
      },
      "source": [
        "https://towardsdatascience.com/machine-learning-with-spark-f1dbc1363986"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkiXSvGtrHum"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}